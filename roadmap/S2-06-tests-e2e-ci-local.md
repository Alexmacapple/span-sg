---
bmad_phase: context-engineered-development
bmad_agent: dev
story_type: implementation
autonomous: true
validation: human-qa
---

# Story S2-06 : Tests E2E automatis√©s et CI local

**Phase** : Semaine 2 - Automatisation
**Priorit√©** : Haute
**Estimation** : 2h30
**Assign√©** : Alexandra

---

## Contexte projet

Le projet SPAN SG dispose actuellement de :
- **Tests unitaires** (S1-05, S1-06) : 7+8 = 15 tests validant composants isol√©s
- **Tests unitaires** (S2-05) : 18 tests pytest pour `calculate_scores.py` + `enrich_pdf_metadata.py`
- **CI GitHub Actions** (S2-01) : Workflow automatique sur push

**Score actuel** : Robustesse 17/20

**Gaps identifi√©s** :
1. **Tests E2E manquants (-2 points)** : Aucun test simulant le workflow complet utilisateur (edit‚Üícommit‚ÜíCI‚Üídeploy‚Üípreview)
2. **Tests CI locaux absents (-1 point)** : Impossible de valider GitHub Actions avant push, risque d'erreurs en CI

**Risques** :
- R√©gressions non d√©tect√©es lors de modifications multi-modules
- √âchecs CI d√©couverts apr√®s push (perte de temps, pollution historique)
- Aucune validation du d√©ploiement gh-pages en environnement simul√©

**B√©n√©fices attendus** :
- D√©tection pr√©coce des r√©gressions (avant CI distante)
- Validation workflow complet (scoring ‚Üí build ‚Üí PDF ‚Üí deploy)
- Confidence pour Semaine 3 (5 contributeurs simultan√©s)
- **Score cible** : 17/20 ‚Üí 20/20 (+3 points robustesse)

---

## Objectif

Impl√©menter des tests end-to-end automatis√©s simulant le workflow complet SPAN SG, et configurer `act` pour ex√©cuter GitHub Actions en local avant push.

---

## Pr√©requis

- [x] Story S1-05 compl√©t√©e (script scoring valid√©)
- [x] Story S1-06 compl√©t√©e (SIRCOM valid√©)
- [x] Story S2-01 compl√©t√©e (CI GitHub Actions op√©rationnelle)
- [x] Story S2-05 compl√©t√©e (tests unitaires pytest)
- [ ] Docker install√© et d√©marr√© (requis pour act + tests E2E preview HTTP)
- [ ] Homebrew install√© (macOS, requis pour installation act)

---

## √âtape 0 : V√©rification pr√©requis (5 min)

### Contexte

Avant d'impl√©menter les tests E2E et CI locaux, v√©rifier que l'environnement dispose des pr√©requis n√©cessaires.

### Script de v√©rification

Cr√©er `scripts/check_prerequisites.sh` :

```bash
#!/usr/bin/env bash
# V√©rification pr√©requis S2-06

set -euo pipefail

echo "=== V√©rification pr√©requis S2-06 ==="
echo ""

ERRORS=0

# Test 1 : Docker install√©
echo -n "Docker install√©... "
if command -v docker &> /dev/null; then
    echo "‚úÖ OK"
else
    echo "‚ùå FAIL"
    echo "   ‚Üí Installer Docker Desktop : https://www.docker.com/products/docker-desktop"
    ERRORS=$((ERRORS + 1))
fi

# Test 2 : Docker d√©marr√©
echo -n "Docker d√©marr√©... "
if docker info > /dev/null 2>&1; then
    echo "‚úÖ OK"
else
    echo "‚ùå FAIL"
    echo "   ‚Üí Lancer Docker Desktop"
    ERRORS=$((ERRORS + 1))
fi

# Test 3 : Homebrew install√© (macOS uniquement)
if [[ "$OSTYPE" == "darwin"* ]]; then
    echo -n "Homebrew install√©... "
    if command -v brew &> /dev/null; then
        echo "‚úÖ OK"
    else
        echo "‚ùå FAIL"
        echo "   ‚Üí Installer Homebrew : https://brew.sh"
        ERRORS=$((ERRORS + 1))
    fi
fi

# Test 4 : act install√© (optionnel, sera install√© en Partie 2)
echo -n "act install√© (optionnel)... "
if command -v act &> /dev/null; then
    echo "‚úÖ OK"
else
    echo "‚è≠Ô∏è  √Ä installer (Partie 2)"
fi

echo ""
if [ $ERRORS -eq 0 ]; then
    echo "‚úÖ Tous les pr√©requis sont satisfaits"
    exit 0
else
    echo "‚ùå $ERRORS pr√©requis manquant(s)"
    echo ""
    echo "Corriger les pr√©requis avant de continuer l'impl√©mentation S2-06."
    exit 1
fi
```

### Ex√©cution

```bash
chmod +x scripts/check_prerequisites.sh
./scripts/check_prerequisites.sh
```

**Attendu** : Tous pr√©requis ‚úÖ OK sauf act (optionnel).

---

## Partie 1 : Tests E2E automatis√©s (1h30)

### Contexte

Les tests E2E simulent le workflow complet d'un contributeur :
1. √âditer un module (cocher/d√©cocher points DINUM)
2. Lancer le scoring
3. V√©rifier synthese.md mis √† jour
4. Builder le site MkDocs
5. G√©n√©rer le PDF
6. V√©rifier artefacts (HTML + PDF)
7. Simuler d√©ploiement
8. Tester rollback (restauration √©tat initial)

**Technologies** :
- Bash scripts (portable, pas de d√©pendances Python)
- Tests isol√©s dans `tests/e2e/`
- Exit codes standardis√©s (0=OK, 1=FAIL)

---

### √âtape 1.1 : Cr√©er structure tests E2E

```bash
mkdir -p tests/e2e
touch tests/e2e/test_full_workflow.sh
chmod +x tests/e2e/test_full_workflow.sh
```

---

### √âtape 1.2 : Script principal E2E

Cr√©er `tests/e2e/test_full_workflow.sh` :

```bash
#!/usr/bin/env bash
set -euo pipefail

# Tests E2E SPAN SG - Workflow complet
# Usage: ./tests/e2e/test_full_workflow.sh

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

TEMP_DIR=$(mktemp -d)
trap "rm -rf $TEMP_DIR" EXIT

echo "=== Tests E2E SPAN SG ==="
echo "Temp dir: $TEMP_DIR"
echo ""

# Compteurs
TESTS_RUN=0
TESTS_PASS=0
TESTS_FAIL=0

run_test() {
    local name=$1
    local command=$2
    TESTS_RUN=$((TESTS_RUN + 1))
    echo -n "Test $TESTS_RUN: $name... "
    if eval "$command" > "$TEMP_DIR/test_$TESTS_RUN.log" 2>&1; then
        echo "‚úÖ PASS"
        TESTS_PASS=$((TESTS_PASS + 1))
    else
        echo "‚ùå FAIL"
        cat "$TEMP_DIR/test_$TESTS_RUN.log"
        TESTS_FAIL=$((TESTS_FAIL + 1))
    fi
}

# Test 1 : Backup module SIRCOM
run_test "Backup module SIRCOM" \
    "cp docs/modules/sircom.md $TEMP_DIR/sircom-backup.md"

# Test 2 : Modifier SIRCOM (cocher 1 point)
run_test "Modifier SIRCOM (cocher point 7)" \
    "sed -i '' 's/- \[ \] Budget annuel d√©di√©/- [x] Budget annuel d√©di√©/' docs/modules/sircom.md"

# Test 3 : Recalculer scores
run_test "Recalculer scores" \
    "python3 scripts/calculate_scores.py"

# Test 4 : V√©rifier score SIRCOM augment√© (base 7/31 + 1 = 8/31)
run_test "V√©rifier score SIRCOM = 8/31" \
    "grep -q '| SIRCOM | 8/31' docs/synthese.md"

# Test 5 : Build site MkDocs
run_test "Build site MkDocs" \
    "mkdocs build --site-dir $TEMP_DIR/site"

# Test 6 : V√©rifier HTML g√©n√©r√©
run_test "V√©rifier index.html pr√©sent" \
    "test -f $TEMP_DIR/site/index.html"

run_test "V√©rifier page SIRCOM g√©n√©r√©e" \
    "test -f $TEMP_DIR/site/modules/sircom/index.html"

# Test 7 : G√©n√©rer PDF
run_test "G√©n√©rer PDF" \
    "mkdocs build --config-file mkdocs-pdf.yml --site-dir $TEMP_DIR/pdf-site"

run_test "V√©rifier PDF g√©n√©r√©" \
    "test -f $TEMP_DIR/pdf-site/exports/span-sg.pdf"

# Test 8 : V√©rifier contenu PDF (au moins 50 KB)
run_test "V√©rifier taille PDF > 50KB" \
    "[[ \$(stat -c%s $TEMP_DIR/pdf-site/exports/span-sg.pdf 2>/dev/null || stat -f%z $TEMP_DIR/pdf-site/exports/span-sg.pdf) -gt 51200 ]]"

# Test 9 : Restaurer SIRCOM
run_test "Restaurer SIRCOM" \
    "mv $TEMP_DIR/sircom-backup.md docs/modules/sircom.md"

# Test 10 : Re-calculer scores (retour √©tat initial)
run_test "Re-calculer scores apr√®s restauration" \
    "python3 scripts/calculate_scores.py"

# Test 11 : V√©rifier score SIRCOM restaur√© (retour √† 7/31)
run_test "V√©rifier score SIRCOM = 7/31 (restaur√©)" \
    "grep -q '| SIRCOM | 7/31' docs/synthese.md"

# R√©sum√©
echo ""
echo "=== R√©sum√© E2E ==="
echo "Tests ex√©cut√©s : $TESTS_RUN"
echo "Tests r√©ussis  : $TESTS_PASS"
echo "Tests √©chou√©s  : $TESTS_FAIL"

if [ $TESTS_FAIL -eq 0 ]; then
    echo "‚úÖ Tous les tests E2E passent"
    exit 0
else
    echo "‚ùå $TESTS_FAIL test(s) √©chou√©(s)"
    exit 1
fi
```

---

### √âtape 1.3 : Tests E2E sc√©narios multiples

Cr√©er 8 sc√©narios E2E suppl√©mentaires :

#### Sc√©nario 1 : Modification multi-modules

`tests/e2e/scenario_multi_modules.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : Modifier 3 modules simultan√©ment

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : Modification multi-modules"

# Backup
cp docs/modules/{sircom,snum,srh}.md /tmp/

# Modifier SIRCOM (cocher point 8)
sed -i '' 's/- \[ \] Planification pluriannuelle/- [x] Planification pluriannuelle/' docs/modules/sircom.md

# Modifier SNUM (cocher point 1)
sed -i '' '0,/- \[ \].*<!-- DINUM -->/s//- [x] Strat√©gie num√©rique publi√©e <!-- DINUM -->/' docs/modules/snum.md

# Modifier SRH (cocher point 1)
sed -i '' '0,/- \[ \].*<!-- DINUM -->/s//- [x] Strat√©gie num√©rique publi√©e <!-- DINUM -->/' docs/modules/srh.md

# Recalculer
python3 scripts/calculate_scores.py

# V√©rifier scores
grep -q "| SIRCOM | 8/31" docs/synthese.md || { echo "FAIL: SIRCOM"; exit 1; }
grep -q "| SNUM | 1/31" docs/synthese.md || { echo "FAIL: SNUM"; exit 1; }
grep -q "| SRH | 1/31" docs/synthese.md || { echo "FAIL: SRH"; exit 1; }
# TOTAL = SIRCOM (7‚Üí8) + SNUM (0‚Üí1) + SRH (0‚Üí1) + autres (0) = 8+1+1 = 10/186
grep -q "| \*\*TOTAL\*\* | \*\*10/186" docs/synthese.md || { echo "FAIL: TOTAL"; exit 1; }

# Restaurer
mv /tmp/{sircom,snum,srh}.md docs/modules/
python3 scripts/calculate_scores.py

echo "‚úÖ Sc√©nario multi-modules OK"
```

#### Sc√©nario 2 : Erreur p√©rim√®tre (exit 2)

`tests/e2e/scenario_erreur_perimetre.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : D√©tecter erreur p√©rim√®tre (‚â† 31 points)

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : Erreur p√©rim√®tre"

# Backup
cp docs/modules/sircom.md /tmp/

# Supprimer 1 checkbox DINUM (31 ‚Üí 30) avec awk
awk '/- \[[ x]\].* DINUM/ && !done {done=1; next} {print}' docs/modules/sircom.md > /tmp/sircom-30.md
mv /tmp/sircom-30.md docs/modules/sircom.md

# Tenter scoring (doit √©chouer avec exit 2)
set +e
python3 scripts/calculate_scores.py 2>&1
EXIT_CODE=$?
set -e

if [ $EXIT_CODE -eq 0 ]; then
    echo "‚ùå FAIL: Scoring devrait √©chouer avec exit 2"
    mv /tmp/sircom.md docs/modules/
    exit 1
fi

# V√©rifier exit code = 2
if [ $EXIT_CODE -ne 2 ]; then
    echo "‚ùå FAIL: Exit code attendu=2, obtenu=$EXIT_CODE"
    mv /tmp/sircom.md docs/modules/
    exit 1
fi

# Restaurer
mv /tmp/sircom.md docs/modules/
python3 scripts/calculate_scores.py

echo "‚úÖ Sc√©nario erreur p√©rim√®tre OK (exit 2 d√©tect√©)"
```

#### Sc√©nario 3 : Build avec erreur Markdown

`tests/e2e/scenario_erreur_markdown.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : D√©tecter erreur Markdown (lien cass√© en mode strict)

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : Erreur Markdown (lien cass√©)"

# Backup
cp docs/modules/sircom.md /tmp/

# Injecter lien cass√©
echo "[Lien cass√©](page-inexistante.md)" >> docs/modules/sircom.md

# Tenter build strict (doit √©chouer)
if mkdocs build --strict 2>/dev/null; then
    echo "‚ùå FAIL: Build strict devrait √©chouer"
    mv /tmp/sircom.md docs/modules/
    exit 1
fi

# Restaurer
mv /tmp/sircom.md docs/modules/

echo "‚úÖ Sc√©nario erreur Markdown OK (strict mode d√©tect√©)"
```

#### Sc√©nario 4 : Performance build

`tests/e2e/scenario_performance.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : V√©rifier temps de build < 10s

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : Performance build"

TEMP_DIR=$(mktemp -d)
trap "rm -rf $TEMP_DIR" EXIT

# Mesurer temps build
START=$(date +%s)
mkdocs build --site-dir "$TEMP_DIR/site" > /dev/null 2>&1
END=$(date +%s)

DURATION=$((END - START))

if [ $DURATION -gt 10 ]; then
    echo "‚ùå FAIL: Build trop long ($DURATION s > 10s)"
    exit 1
fi

echo "‚úÖ Sc√©nario performance OK (build en ${DURATION}s)"
```

#### Sc√©nario 5 : Validation PDF complet

`tests/e2e/scenario_pdf_complet.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : V√©rifier PDF contient toutes les pages

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : PDF complet"

TEMP_DIR=$(mktemp -d)
trap "rm -rf $TEMP_DIR" EXIT

# G√©n√©rer PDF
mkdocs build --config-file mkdocs-pdf.yml --site-dir "$TEMP_DIR/pdf-site" > /dev/null 2>&1

PDF_FILE="$TEMP_DIR/pdf-site/exports/span-sg.pdf"

# V√©rifier pr√©sence
test -f "$PDF_FILE" || { echo "‚ùå FAIL: PDF absent"; exit 1; }

# V√©rifier taille > 100 KB (indicateur contenu complet)
SIZE=$(stat -c%s "$PDF_FILE" 2>/dev/null || stat -f%z "$PDF_FILE")
if [ $SIZE -lt 102400 ]; then
    echo "‚ùå FAIL: PDF trop petit ($SIZE bytes < 100KB)"
    exit 1
fi

# V√©rifier contenu avec pdfinfo (si disponible)
if command -v pdfinfo &> /dev/null; then
    PAGES=$(pdfinfo "$PDF_FILE" | grep "^Pages:" | awk '{print $2}')
    if [ "$PAGES" -lt 10 ]; then
        echo "‚ùå FAIL: PDF incomplet ($PAGES pages < 10)"
        exit 1
    fi
    echo "‚úÖ Sc√©nario PDF complet OK ($PAGES pages, ${SIZE} bytes)"
else
    echo "‚úÖ Sc√©nario PDF complet OK (${SIZE} bytes, pdfinfo non disponible)"
fi
```

#### Sc√©nario 6 : Rollback complet

`tests/e2e/scenario_rollback.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : Tester rollback complet (all modules)

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : Rollback complet"

# Backup tous modules
cp -r docs/modules /tmp/modules-backup

# Modifier tous modules (cocher 1er point partout)
for module in snum sircom srh siep safi bgs; do
    sed -i '' '0,/- \[ \].*<!-- DINUM -->/s//- [x] Point coch√© <!-- DINUM -->/' "docs/modules/$module.md"
done

# Recalculer
python3 scripts/calculate_scores.py

# V√©rifier TOTAL apr√®s modification 6 modules
# Calcul : SIRCOM (7‚Üí8) + SNUM (0‚Üí1) + SRH (0‚Üí1) + SIEP (0‚Üí1) + SAFI (0‚Üí1) + BGS (0‚Üí1)
#        = 8 + 1 + 1 + 1 + 1 + 1 = 13/186
grep -q "13/186" docs/synthese.md || { echo "‚ùå FAIL: Score incorrect"; cat docs/synthese.md; exit 1; }

# Rollback
rm -rf docs/modules
mv /tmp/modules-backup docs/modules
python3 scripts/calculate_scores.py

# V√©rifier retour √† 7/186
grep -q "7/186" docs/synthese.md || { echo "‚ùå FAIL: Rollback incorrect"; exit 1; }

echo "‚úÖ Sc√©nario rollback OK"
```

#### Sc√©nario 7 : Preview HTTP local (Docker avec libsass)

**Pr√©requis** : Cr√©er `Dockerfile.mkdocs-test` avec build tools pour compilation libsass.

`Dockerfile.mkdocs-test` :

```dockerfile
FROM squidfunk/mkdocs-material:latest

# Installer build-essentials pour compiler libsass (Alpine Linux)
RUN apk add --no-cache \
    gcc \
    g++ \
    musl-dev \
    python3-dev

WORKDIR /docs
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

CMD ["mkdocs", "serve", "--dev-addr=0.0.0.0:8000"]
```

`tests/e2e/scenario_preview_http.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : V√©rifier preview HTTP local (Docker)

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : Preview HTTP local"

# V√©rifier Docker running
if ! docker info > /dev/null 2>&1; then
    echo "‚ö†Ô∏è  SKIP: Docker non d√©marr√©"
    exit 0
fi

# Build image avec gcc/g++ pour libsass
docker build -f Dockerfile.mkdocs-test -t span-mkdocs-test . > /dev/null 2>&1 || {
    echo "‚ö†Ô∏è  SKIP: Build Docker √©chou√©"
    exit 0
}

# D√©marrer container mkdocs serve (background)
CONTAINER_ID=$(docker run -d -p 8000:8000 -v "$(pwd):/docs" span-mkdocs-test)

# Attendre 5s pour d√©marrage
sleep 5

# Test HTTP
HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/)

if [ "$HTTP_CODE" != "200" ]; then
    echo "‚ùå FAIL: HTTP code = $HTTP_CODE (attendu 200)"
    docker stop "$CONTAINER_ID" > /dev/null 2>&1
    exit 1
fi

# Test page SIRCOM
HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/modules/sircom/)

if [ "$HTTP_CODE" != "200" ]; then
    echo "‚ùå FAIL: Page SIRCOM HTTP = $HTTP_CODE"
    docker stop "$CONTAINER_ID" > /dev/null 2>&1
    exit 1
fi

# Cleanup
docker stop "$CONTAINER_ID" > /dev/null 2>&1

echo "‚úÖ Sc√©nario preview HTTP OK"
```

#### Sc√©nario 8 : Validation front-matter YAML

`tests/e2e/scenario_frontmatter.sh` :

```bash
#!/usr/bin/env bash
# Sc√©nario : Valider front-matter YAML tous modules

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "Sc√©nario : Validation front-matter YAML"

for module in snum sircom srh siep safi bgs; do
    MODULE_FILE="docs/modules/$module.md"

    # Extraire front-matter (entre --- et ---)
    FRONTMATTER=$(sed -n '/^---$/,/^---$/p' "$MODULE_FILE" | sed '1d;$d')

    # Valider YAML avec Python
    python3 -c "
import yaml
import sys
try:
    data = yaml.safe_load('''$FRONTMATTER''')
    assert 'service' in data, 'Cl√© service manquante'
    assert 'referent' in data, 'Cl√© referent manquante'
    assert 'updated' in data, 'Cl√© updated manquante'
except Exception as e:
    print(f'‚ùå FAIL {module}: {e}')
    sys.exit(1)
" || exit 1

    echo "  ‚úÖ $module : YAML valide"
done

echo "‚úÖ Sc√©nario front-matter OK (6 modules)"
```

---

### √âtape 1.4 : Runner tous sc√©narios E2E

Cr√©er `tests/e2e/run_all.sh` :

```bash
#!/usr/bin/env bash
# Runner tous tests E2E

set -euo pipefail
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "=== Runner Tests E2E SPAN SG ==="
echo ""

TESTS=(
    "tests/e2e/test_full_workflow.sh"
    "tests/e2e/scenario_multi_modules.sh"
    "tests/e2e/scenario_erreur_perimetre.sh"
    "tests/e2e/scenario_erreur_markdown.sh"
    "tests/e2e/scenario_performance.sh"
    "tests/e2e/scenario_pdf_complet.sh"
    "tests/e2e/scenario_rollback.sh"
    "tests/e2e/scenario_preview_http.sh"
    "tests/e2e/scenario_frontmatter.sh"
)

PASS=0
FAIL=0
SKIP=0

for test in "${TESTS[@]}"; do
    chmod +x "$test"
    echo "Ex√©cution : $test"
    if "$test"; then
        PASS=$((PASS + 1))
    else
        EXIT_CODE=$?
        if [ $EXIT_CODE -eq 77 ]; then
            SKIP=$((SKIP + 1))
        else
            FAIL=$((FAIL + 1))
        fi
    fi
    echo ""
done

echo "=== R√©sum√© Global E2E ==="
echo "Tests r√©ussis : $PASS"
echo "Tests √©chou√©s : $FAIL"
echo "Tests skipp√©s : $SKIP"

if [ $FAIL -eq 0 ]; then
    echo "‚úÖ Tous les tests E2E passent"
    exit 0
else
    echo "‚ùå $FAIL test(s) √©chou√©(s)"
    exit 1
fi
```

---

## Partie 2 : Tests CI locaux avec `act` (1h)

### Contexte

`act` (nektos/act) permet d'ex√©cuter GitHub Actions workflows en local avec Docker, √©vitant :
- Push pour tester CI (gain temps + historique propre)
- Consommation quota GitHub Actions (2000 min/mois)
- Cycles debug longs (push ‚Üí attendre CI ‚Üí corriger ‚Üí push)

**Workflow type** :
```bash
act -j build        # Ex√©cuter job "build" seulement
act -l              # Lister jobs disponibles
act push            # Simuler event "push"
```

---

### √âtape 2.1 : Installer `act`

#### macOS (Homebrew)

```bash
brew install act
```

#### Linux

```bash
curl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash
```

#### V√©rifier installation

```bash
act --version
# act version 0.2.xx
```

---

### √âtape 2.2 : Configuration `act`

Cr√©er `.actrc` √† la racine du projet :

```bash
# .actrc - Configuration act pour SPAN SG

# Image Docker medium (inclut Python)
-P ubuntu-latest=catthehacker/ubuntu:act-latest

# Secrets locaux
--secret-file .secrets

# Variables d'environnement
--env GITHUB_REPOSITORY=Alexmacapple/span-sg-repo
--env GITHUB_REF=refs/heads/draft

# Verbose
-v
```

---

### √âtape 2.3 : Fichier secrets locaux

Cr√©er `.secrets` (√† NE PAS commiter) :

```bash
GITHUB_TOKEN=ghp_fake_token_for_local_testing
```

Ajouter `.secrets` au `.gitignore` :

```bash
echo ".secrets" >> .gitignore
```

---

### √âtape 2.4 : Adapter workflow pour `act`

Le workflow `.github/workflows/build.yml` doit supporter `act`. V√©rifier :

```yaml
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Compatible act : pas de d√©pendance externe
      - name: Install dependencies
        run: |
          pip install mkdocs-material mkdocs-pdf-export-plugin

      - name: Calculate SPAN scores
        run: python scripts/calculate_scores.py

      - name: Build site
        run: mkdocs build

      - name: Generate PDF
        run: mkdocs build --config-file mkdocs-pdf.yml

      # Skip upload artifacts en local (act limitation)
      - name: Upload artifacts
        if: ${{ !env.ACT }}
        uses: actions/upload-artifact@v3
        with:
          name: span-site
          path: |
            site/
            exports/
```

**Astuce** : `if: ${{ !env.ACT }}` d√©tecte ex√©cution locale et skip steps incompatibles.

---

### √âtape 2.5 : Tester workflow complet en local

```bash
# Tester job "build" uniquement
act -j build

# Simuler push sur draft
act push --ref refs/heads/draft

# Dry-run (lister steps sans ex√©cuter)
act -j build --dryrun

# Debug mode (verbose)
act -j build -v
```

**Attendu** :
```
[Build SPAN/build] üöÄ Start image=catthehacker/ubuntu:act-latest
[Build SPAN/build]   ‚úÖ Set up Python
[Build SPAN/build]   ‚úÖ Install dependencies
[Build SPAN/build]   ‚úÖ Calculate SPAN scores
[Build SPAN/build]   ‚úÖ Build site
[Build SPAN/build]   ‚úÖ Generate PDF
[Build SPAN/build] ‚úÖ Success
```

---

### √âtape 2.6 : Script wrapper `test-ci-local.sh`

Cr√©er `scripts/test-ci-local.sh` :

```bash
#!/usr/bin/env bash
# Test GitHub Actions en local avec act

set -euo pipefail

echo "=== Test CI local avec act ==="

# V√©rifier act install√©
if ! command -v act &> /dev/null; then
    echo "‚ùå act non install√©. Installer avec: brew install act"
    exit 1
fi

# V√©rifier Docker running
if ! docker info > /dev/null 2>&1; then
    echo "‚ùå Docker non d√©marr√©. Lancer Docker Desktop."
    exit 1
fi

# Lister jobs disponibles
echo "Jobs disponibles :"
act -l

echo ""
echo "Ex√©cution job 'build' :"

# Ex√©cuter job build
if act -j build; then
    echo "‚úÖ CI local r√©ussie"
    exit 0
else
    echo "‚ùå CI local √©chou√©e"
    exit 1
fi
```

Rendre ex√©cutable :

```bash
chmod +x scripts/test-ci-local.sh
```

---

### √âtape 2.7 : Sc√©narios validation CI local

#### Sc√©nario 1 : Validation job build

```bash
# Test complet
act -j build

# Attendu : Exit 0, tous steps ‚úÖ
```

#### Sc√©nario 2 : Validation job deploy_draft (simulation)

```bash
# Simuler push sur draft
act push --ref refs/heads/draft -j deploy_draft

# Attendu : Job deploy_draft s'ex√©cute
```

#### Sc√©nario 3 : Test erreur scoring

Modifier temporairement SIRCOM (supprimer 1 DINUM), puis :

```bash
act -j build

# Attendu : Step "Calculate SPAN scores" √©choue, exit 2
```

Restaurer SIRCOM apr√®s test.

#### Sc√©nario 4 : Test sans Docker (skip)

Si Docker non disponible :

```bash
if docker info > /dev/null 2>&1; then
    act -j build
else
    echo "‚ö†Ô∏è  SKIP: Docker requis pour act"
    exit 0
fi
```

#### Sc√©nario 5 : Validation performance CI locale

```bash
START=$(date +%s)
act -j build
END=$(date +%s)

DURATION=$((END - START))

if [ $DURATION -gt 300 ]; then
    echo "‚ö†Ô∏è  CI locale lente ($DURATION s > 5 min)"
else
    echo "‚úÖ CI locale rapide ($DURATION s)"
fi
```

---

## Partie 3 : Int√©gration et documentation (30 min)

### √âtape 3.1 : Documentation tests E2E et CI local

Cr√©er `tests/README.md` :

```markdown
# Tests SPAN SG

Ce dossier contient les tests end-to-end (E2E) et les outils de validation CI locale.

## Tests E2E

**Objectif** : Valider le workflow complet (edit ‚Üí scoring ‚Üí build ‚Üí PDF ‚Üí deploy)

### Ex√©cution

```bash
# Test principal
./tests/e2e/test_full_workflow.sh

# Tous sc√©narios
./tests/e2e/run_all.sh

# Sc√©nario sp√©cifique
./tests/e2e/scenario_multi_modules.sh
```

### Sc√©narios disponibles

1. **test_full_workflow.sh** : Workflow complet (11 tests)
2. **scenario_multi_modules.sh** : Modification simultan√©e 3 modules
3. **scenario_erreur_perimetre.sh** : D√©tection erreur p√©rim√®tre (exit 2)
4. **scenario_erreur_markdown.sh** : D√©tection lien cass√© (strict mode)
5. **scenario_performance.sh** : Build < 10s
6. **scenario_pdf_complet.sh** : PDF complet > 100 KB
7. **scenario_rollback.sh** : Rollback complet tous modules
8. **scenario_preview_http.sh** : Preview HTTP Docker
9. **scenario_frontmatter.sh** : Validation YAML 6 modules

### Int√©gration CI

Les tests E2E sont ex√©cut√©s automatiquement par GitHub Actions :

```yaml
- name: Run E2E tests
  run: ./tests/e2e/run_all.sh
```

## Tests CI locaux avec `act`

**Objectif** : Valider GitHub Actions avant push

### Installation

```bash
brew install act  # macOS
```

### Configuration

Fichier `.actrc` √† la racine :

```bash
-P ubuntu-latest=catthehacker/ubuntu:act-latest
--secret-file .secrets
-v
```

Cr√©er `.secrets` (ne pas commiter) :

```bash
GITHUB_TOKEN=ghp_fake_token
```

### Utilisation

```bash
# Tester job build
act -j build

# Lister jobs
act -l

# Simuler push draft
act push --ref refs/heads/draft

# Script wrapper
./scripts/test-ci-local.sh
```

### Troubleshooting

**Erreur "Docker daemon not running"** :
- Lancer Docker Desktop

**Erreur "image not found"** :
- Utiliser image medium : `-P ubuntu-latest=catthehacker/ubuntu:act-latest`

**Erreur "secrets not found"** :
- Cr√©er `.secrets` avec `GITHUB_TOKEN=fake_token`

**Step √©choue en local mais OK en CI** :
- V√©rifier compatibilit√© action avec act
- Utiliser `if: ${{ !env.ACT }}` pour skip step

## Pr√©-commit hook (optionnel)

Ex√©cuter tests E2E avant commit :

`.git/hooks/pre-commit` :

```bash
#!/bin/bash
./tests/e2e/test_full_workflow.sh
```

Activer :

```bash
chmod +x .git/hooks/pre-commit
```
```

---

### √âtape 3.2 : Ajouter tests E2E √† CI

Modifier `.github/workflows/build.yml` pour inclure tests E2E :

```yaml
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install mkdocs-material mkdocs-pdf-export-plugin

      # AJOUT : Tests E2E avant scoring
      - name: Run E2E tests
        run: |
          chmod +x tests/e2e/run_all.sh
          ./tests/e2e/run_all.sh

      - name: Calculate SPAN scores
        run: python scripts/calculate_scores.py

      # ... reste du workflow
```

---

### √âtape 3.3 : Ajouter badge tests E2E

Cr√©er badge dans README.md :

```markdown
# SPAN SG

![Build Status](https://github.com/Alexmacapple/span-sg-repo/workflows/Build%20SPAN/badge.svg)
![E2E Tests](https://img.shields.io/badge/E2E%20tests-9%20scenarios-success)
![CI Local](https://img.shields.io/badge/CI%20local-act%20compatible-blue)

...
```

---

## Crit√®res d'acceptation

- [ ] R√©pertoire `tests/e2e/` cr√©√© avec 9 sc√©narios
- [ ] Script principal `test_full_workflow.sh` ex√©cute 11 tests
- [ ] Runner `run_all.sh` ex√©cute tous sc√©narios
- [ ] `act` install√© et fonctionnel
- [ ] `.actrc` configur√© avec image medium
- [ ] `.secrets` cr√©√© (exclu du git)
- [ ] Workflow `.github/workflows/build.yml` compatible `act`
- [ ] Script `scripts/test-ci-local.sh` fonctionnel
- [ ] Documentation `tests/README.md` compl√®te
- [ ] Tests E2E int√©gr√©s √† CI GitHub Actions
- [ ] Tous sc√©narios E2E passent (9/9)
- [ ] CI locale passe avec `act -j build`

---

## Tests de validation

```bash
# Test 1 : Structure tests E2E pr√©sente
test -d tests/e2e && echo "OK" || echo "FAIL"
# Attendu : OK

# Test 2 : 9 sc√©narios E2E pr√©sents
test $(ls tests/e2e/*.sh | wc -l) -ge 9 && echo "OK" || echo "FAIL"
# Attendu : OK

# Test 3 : Tous scripts ex√©cutables
find tests/e2e -name "*.sh" -executable | wc -l
# Attendu : 9

# Test 4 : act install√©
command -v act && echo "OK" || echo "FAIL"
# Attendu : OK

# Test 5 : .actrc pr√©sent
test -f .actrc && echo "OK" || echo "FAIL"
# Attendu : OK

# Test 6 : .secrets exclu du git
grep -q "^\.secrets$" .gitignore && echo "OK" || echo "FAIL"
# Attendu : OK

# Test 7 : Workflow compatible act (d√©tection env.ACT)
grep -q "env.ACT" .github/workflows/build.yml && echo "OK" || echo "FAIL"
# Attendu : OK

# Test 8 : Documentation pr√©sente
test -f tests/README.md && echo "OK" || echo "FAIL"
# Attendu : OK

# Test 9 : Runner E2E passe
./tests/e2e/run_all.sh && echo "OK" || echo "FAIL"
# Attendu : OK (9/9 tests)

# Test 10 : CI locale passe (si Docker running)
if docker info > /dev/null 2>&1; then
    act -j build --dryrun && echo "OK" || echo "FAIL"
else
    echo "SKIP (Docker requis)"
fi
# Attendu : OK ou SKIP
```

---

## D√©pendances

**Bloque** :
- S3-01 (modules vides n√©cessitent tests E2E avant onboarding)
- S4-04 (publication n√©cessite validation E2E compl√®te)

**D√©pend de** :
- S1-05 (script scoring doit fonctionner)
- S2-01 (CI GitHub Actions doit √™tre op√©rationnelle)
- S2-05 (tests unitaires pytest doivent passer)

---

## R√©f√©rences

- **PRD v3.3** : Section 11 "Plan de mise en ≈ìuvre" ‚Üí Semaine 2 Automatisation
- **.github/workflows/build.yml** : Workflow CI √† tester
- **scripts/calculate_scores.py** : Script test√© en E2E
- **nektos/act** : https://github.com/nektos/act
- **GitHub Actions** : https://docs.github.com/en/actions

---

## Notes et risques

### Performance `act`

Premi√®re ex√©cution `act` t√©l√©charge image Docker (~2 GB) :
```bash
docker pull catthehacker/ubuntu:act-latest
```

Ex√©cutions suivantes : cache Docker utilis√©, rapide.

### Limitations `act`

Actions non support√©es :
- `actions/upload-artifact` (skip avec `if: ${{ !env.ACT }}`)
- `actions/deploy-pages` (skip)
- Secrets GitHub natifs (utiliser `.secrets` local)

### Faux positifs E2E

Test `scenario_preview_http.sh` peut √©chouer si :
- Docker non d√©marr√©
- Port 8000 d√©j√† utilis√©
- Service mkdocs non configur√© dans `docker-compose.yml`

‚Üí Utiliser `exit 0` (skip) au lieu de `exit 1` si Docker indisponible.

### Co√ªt maintenance

9 sc√©narios E2E √† maintenir lors de modifications :
- Ajout module ‚Üí Adapter `scenario_multi_modules.sh`
- Changement scoring ‚Üí Adapter tous tests v√©rifiant scores
- Changement structure ‚Üí Adapter `scenario_frontmatter.sh`

**Mitigation** : Tests E2E g√©n√©riques (boucles sur modules) vs hardcod√©s.

### Quota CI

Avec tests E2E en CI :
- Dur√©e totale workflow : 2-3 min ‚Üí 4-5 min (E2E +2 min)
- Quota GitHub Actions : 2000 min/mois ‚Üí ~400 runs/mois (au lieu de 600)

**Mitigation** : Tests E2E optionnels sur PR, obligatoires sur main/draft.

---

## Post-t√¢che

### Cr√©er issue suivi

```markdown
## Tests E2E - Suivi am√©liorations

- [ ] Ajouter test E2E pour d√©ploiement gh-pages r√©el
- [ ] Int√©grer tests E2E dans pre-commit hook (optionnel)
- [ ] Cr√©er rapport HTML tests E2E (pytest-html style)
- [ ] Ajouter sc√©nario E2E pour releases (tags)
- [ ] Optimiser temps ex√©cution E2E (parall√©lisation)
```

### M√©triques √† suivre

```bash
# Temps ex√©cution E2E
time ./tests/e2e/run_all.sh
# Objectif : < 3 min

# Taux succ√®s E2E (sur 1 mois)
# Objectif : > 95%

# Temps ex√©cution CI locale
time act -j build
# Objectif : < 5 min
```

### Documentation contributeur

Ajouter √† `CONTRIBUTING.md` (S2-04) :

```markdown
## Validation avant commit

Recommand√© :

```bash
# 1. Tests unitaires
pytest tests/

# 2. Tests E2E
./tests/e2e/run_all.sh

# 3. CI locale
./scripts/test-ci-local.sh

# 4. Commit si tout passe
git add .
git commit -m "..."
```
```

---

**Validation compl√®te** : Cette story porte la robustesse de 17/20 √† 20/20 (+3 points).
